

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>3. Linear regression &mdash; NMSMExp 0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"equationNumbers": {"autoNumber": "AMS", "useLabelIds": true}}, "tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4. Newton methods" href="newton_methods.html" />
    <link rel="prev" title="2. Linear models for regression" href="PRML_ch3_lin_reg.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> NMSMExp
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Linear Algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#probability-and-statistics">Probability and Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#convex-optimization">Convex Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#bayesian-inference">Bayesian Inference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html#machine-learning">Machine Learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="PRML_ch1_intro.html">1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="PRML_ch3_lin_reg.html">2. Linear models for regression</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">3. Linear regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Introduction">3.1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#The-normal-equations">3.2. The normal equations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#matrix-derivatives">3.2.1. matrix derivatives</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-least-squares-solution">3.2.2. the least-squares solution</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Batch-gradient-descent-and-LMS-algorithm">3.3. Batch gradient descent and LMS algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Stochastic-gradient-descent">3.4. Stochastic gradient descent</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#a-remark-on-“learning-rate”-\alpha">3.4.1. a remark on “learning rate” <span class="math notranslate nohighlight">\(\alpha\)</span></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Probabilistic-interpretation">3.5. Probabilistic interpretation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Locally-weighted-linear-regression">3.6. Locally weighted linear regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Linear-regression-using-sckikit-learn">3.7. Linear regression using <code class="docutils literal notranslate"><span class="pre">sckikit-learn</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="newton_methods.html">4. Newton methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="logistic_regression.html">5. Classification and logistic regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="perceptron.html">6. Random linear classifiers</a></li>
<li class="toctree-l2"><a class="reference internal" href="perceptron.html#Perceptrons">7. Perceptrons</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#gaussian-processes">Gaussian Processes</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NMSMExp</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Linear Algebra</a> &raquo;</li>
        
      <li>3. Linear regression</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorial/linear_regression.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;bmh&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">std_fig</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>For these algorithms, we will use data from the Stanford CS 229 problem set 1 dataset.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">data_dir</span> <span class="o">=</span> <span class="s1">&#39;../data/cs229/PS1-data/q2/data/&#39;</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1">#X = np.loadtxt(data_dir + &#39;x.dat&#39;)</span>
<span class="c1">#Y = np.loadtxt(data_dir + &#39;y.dat&#39;)</span>
</pre></div>
</div>
</div>
<div class="section" id="Linear-regression">
<h1>3. Linear regression<a class="headerlink" href="#Linear-regression" title="Permalink to this headline">¶</a></h1>
<div class="section" id="Introduction">
<h2>3.1. Introduction<a class="headerlink" href="#Introduction" title="Permalink to this headline">¶</a></h2>
<p>The idea is that we want to discover the weights <span class="math notranslate nohighlight">\(\theta\)</span> such that</p>
<div class="math notranslate nohighlight">
\[h_\theta(x) = \theta_0 + \theta_1 x_1 +\ldots + \theta_n x_n\]</div>
<p>is a good model for predicting the output <span class="math notranslate nohighlight">\(y\)</span> from an input <span class="math notranslate nohighlight">\(x\)</span>. We can make the notation more compact by defining <span class="math notranslate nohighlight">\(x_0=1\)</span> and writing</p>
<div class="math notranslate nohighlight">
\[h(x) = \theta^T x\]</div>
<p>How do we choose <span class="math notranslate nohighlight">\(\theta\)</span> (and hence, <span class="math notranslate nohighlight">\(h(x)\)</span>)? We define the cost function</p>
<div class="math notranslate nohighlight">
\[J(\theta) = \frac{1}{2} \sum_{i=1}^m \left(h_\theta(x^{(i)}) - y^{(i)}\right)^2\]</div>
<p>This is the familiar <strong>least squares</strong> cost function. There is more than one way to minimize <span class="math notranslate nohighlight">\(J\)</span>; we consider some common algorithms here.</p>
</div>
<div class="section" id="The-normal-equations">
<h2>3.2. The normal equations<a class="headerlink" href="#The-normal-equations" title="Permalink to this headline">¶</a></h2>
<p>The most direct way of minimizing <span class="math notranslate nohighlight">\(J\)</span> involves finding the derivative of <span class="math notranslate nohighlight">\(J(\theta)\)</span> with respect to <span class="math notranslate nohighlight">\(\theta\)</span> and setting it equal to zero. This is how the classical least-squares form is derived, and works because the objective function to be minimized is convex.</p>
<div class="section" id="matrix-derivatives">
<h3>3.2.1. matrix derivatives<a class="headerlink" href="#matrix-derivatives" title="Permalink to this headline">¶</a></h3>
<p>We define the derivative of some function <span class="math notranslate nohighlight">\(f\)</span> with respect to a matrix <span class="math notranslate nohighlight">\(A\in\mathbb{R}^{n\times m}\)</span> to be:</p>
<p><span class="math">\begin{equation}
\nabla_A f(A) \equiv
\left[
\begin{array}
\frac{\partial f}{\partial A_{11}} & \cdots & \frac{\partial f}{\partial A_{1n}} \\
\vdots & \ddots & \vdots \\
\frac{\partial f}{\partial A_{m1}} & \cdots & \frac{\partial f}{\partial A_{mn}},
\end{array}\right]
\end{equation}</span></p>
<p>that is, the derivative of a function with respect to a matrix is the derivative of the function with respect to each entry of the matrix:</p>
<div class="math notranslate nohighlight">
\[\left[\nabla_A f(A)\right]_{ij} = \frac{\partial f}{a_{ij}}.\]</div>
<p>We also make use of the trace operator <span class="math notranslate nohighlight">\(\text{tr} A = \sum_{i=1}^n a_{ii}\)</span>. The trace operator is a symmetric, linear operator. It’s easy enough to check this. We can write out some identities</p>
<p><span class="math">\begin{align}
\nabla_A \text{tr} AB &= B^T \\
\nabla_{A^T} f(A) &= \left(\nabla_A f(A)\right)^T \\
\nabla _A \text{tr} ABA^TC &= CAB + C^T AB^T
\end{align}</span></p>
<p>In the first equation, for example, we have that <span class="math notranslate nohighlight">\(AB\)</span> is square, so we can take its trace. The derivative of this trace with respect to <span class="math notranslate nohighlight">\(A\)</span> is itself a matrix. It will have the same entries as <span class="math notranslate nohighlight">\(B^T\)</span>.</p>
</div>
<div class="section" id="the-least-squares-solution">
<h3>3.2.2. the least-squares solution<a class="headerlink" href="#the-least-squares-solution" title="Permalink to this headline">¶</a></h3>
<p>We can now derive the exact minimizer <span class="math notranslate nohighlight">\(\theta\)</span> of <span class="math notranslate nohighlight">\(J(\theta)\)</span>. We define the <strong>design matrix</strong> <span class="math notranslate nohighlight">\(X\)</span> as the <span class="math notranslate nohighlight">\(m\times (n+1)\)</span> matrix (<span class="math notranslate nohighlight">\(n+1\)</span> for the intercept term) which contains the training examples in each row and the vector <span class="math notranslate nohighlight">\(y\)</span> containing the target values from the training set</p>
<p><span class="math">\begin{equation}
X = \left[
\begin{array}{c}
\left(x^{(1)}\right)^T\\
\vdots \\
(x^{(m)})^T \\
\end{array}
\right],
y = \left[
\begin{array}{c}
y^{(1)} \\
\vdots \\
y^{(m)} \\
\end{array}
\right]
\end{equation}</span></p>
<p>Then</p>
<p><span class="math">\begin{equation}
X\theta - y =
\left[
\begin{array}{c}
(x^{(1)})^T \theta - y^{(1)} \\
\vdots \\
h_\theta(x^{(m)}) - y^{(m)} \\
\end{array}
\right]
\end{equation}</span></p>
<p>and hence</p>
<div class="math notranslate nohighlight">
\[ J(\theta) = \frac{1}{2} \sum_{i=1}^m
\left(h_\theta\left(x^{(i)}\right) - y^{(i)}\right)^2
= \frac{1}{2} (X\theta - y)^T (X\theta - y)\]</div>
<p>Hence <span class="math">\begin{align}
\nabla_\theta J(\theta) &= \frac{1}{2} (X\theta - y)^T (X\theta - y) \\
&= \frac{1}{2} \nabla_\theta \left((X\theta)^T(X\theta) - (X\theta)^T y - y^T X\theta + y^T y\right) \\
&= \frac{1}{2} \nabla_\theta \text{tr}\left((X\theta)^T(X\theta) - (X\theta)^T y - y^T X\theta + y^T y\right) \\
&= \frac{1}{2} \nabla_\theta \left(\text{tr}(X\theta)^T(X\theta) - 2\text{tr } y^T (X\theta) \right) \\
&= \frac{1}{2} \left(2X^TX\theta - 2X^Ty \right)
\end{align}</span></p>
<p>where in the third step, we used the fact that a real number is its own trace. In the last step, we used</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\nabla_\theta \text{tr } \theta^T(X^T X) \theta I  = \left[\nabla_{\theta^T} \text{tr} \underbrace{\theta^T}_{A} \underbrace{(X^T X)}_{B} \underbrace{\theta}_{A^T} \underbrace{I}_C \right] ^T = \left[\theta^T X^TX + \theta^T X^TX\right]^T = 2 X^TX \theta\\and\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[\nabla_\theta \text{tr } y^T(X\theta) = \left[\nabla_{\theta^T} \text{tr }\theta^T(X^Ty)\right]^T = \left[(X^T y)^T\right]^T = X^Ty\]</div>
<p>Therefore analytically requiring that <span class="math notranslate nohighlight">\(\nabla_\theta J(\theta) = 0\)</span> along with the fact that <span class="math notranslate nohighlight">\(J\)</span> is convex and quadratic allows us to solve the minimization problem directly via the <strong>normal equations</strong>:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{2} \left(2X^TX\theta - 2X^Ty \right) = 0 \qquad \Rightarrow \theta = (X^TX)^{-1} X^T y\]</div>
<p>This is the closed-form classical least squares solution, which can be computed directly without need for iteration.</p>
<p>Then <span class="math">\begin{equation}
X\theta - y =
\left[
\begin{array}{c}
( x^{(1)})^T \theta - y^{(1)} \\
\vdots \\
h_\theta(x^{(m)}) - y^{(m)}
\end{array}
\right]
\end{equation}</span></p>
<p>and hence</p>
<div class="math notranslate nohighlight">
\[ J(\theta) = \frac{1}{2} \sum_{i=1}^m
\left(h_\theta\left(x^{(i)}\right) - y^{(i)}\right)^2
= \frac{1}{2} (X\theta - y)^T (X\theta - y)\]</div>
<p>Hence <span class="math">\begin{align}
\nabla_\theta J(\theta) &= \frac{1}{2} (X\theta - y)^T (X\theta - y) \\
&= \frac{1}{2} \nabla_\theta \left((X\theta)^T(X\theta) - (X\theta)^T y - y^T X\theta + y^T y\right) \\
&= \frac{1}{2} \nabla_\theta \text{tr}\left((X\theta)^T(X\theta) - (X\theta)^T y - y^T X\theta + y^T y\right) \\
&= \frac{1}{2} \nabla_\theta \left(\text{tr}(X\theta)^T(X\theta) - 2\text{tr } y^T (X\theta) \right) \\
&= \frac{1}{2} \left(2X^TX\theta - 2X^Ty \right)
\end{align}</span></p>
<p>where in the third step, we used the fact that a real number is its own trace. In the last step, we used</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\nabla_\theta \text{tr } \theta^T(X^T X) \theta I  = \left[\nabla_{\theta^T} \text{tr} \underbrace{\theta^T}_{A} \underbrace{(X^T X)}_{B} \underbrace{\theta}_{A^T} \underbrace{I}_C \right] ^T = \left[\theta^T X^TX + \theta^T X^TX\right]^T = 2 X^TX \theta\\and\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[\nabla_\theta \text{tr } y^T(X\theta) = \left[\nabla_{\theta^T} \text{tr }\theta^T(X^Ty)\right]^T = \left[(X^T y)^T\right]^T = X^Ty\]</div>
<p>Therefore analytically requiring that <span class="math notranslate nohighlight">\(\nabla_\theta J(\theta) = 0\)</span> along with the fact that <span class="math notranslate nohighlight">\(J\)</span> is convex and quadratic allows us to solve the minimization problem directly:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{2} \left(2X^TX\theta - 2X^Ty \right) = 0 \qquad \Rightarrow \theta = (X^TX)^{-1} X^T y\]</div>
<p>This is the closed-form classical least squares solution, which can be computed directly without need for iteration. However, in cases where the matrices in question are sufficiently large or ill-conditioned that evaluating the least squares solution directly is not feasible, we consider other common approaches.</p>
</div>
</div>
<div class="section" id="Batch-gradient-descent-and-LMS-algorithm">
<h2>3.3. Batch gradient descent and LMS algorithm<a class="headerlink" href="#Batch-gradient-descent-and-LMS-algorithm" title="Permalink to this headline">¶</a></h2>
<p>We use a search algorithm to find <span class="math notranslate nohighlight">\(\theta\)</span> which minimizes <span class="math notranslate nohighlight">\(J(\theta)\)</span>. The rough approach starts with an initial guess for <span class="math notranslate nohighlight">\(\theta\)</span> and iterates to make <span class="math notranslate nohighlight">\(J(\theta)\)</span> smaller, until (hopefully) convergence. We consider the <strong>gradient descent</strong> algorithm, which has the update rule</p>
<p><span class="math">\begin{equation}
\theta_{j} := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
\end{equation}</span></p>
<p>This update is performed for all values of <span class="math notranslate nohighlight">\(j\)</span>. Here <span class="math notranslate nohighlight">\(\alpha\)</span> is called the <strong>learning rate</strong>. The algorithm works by taking steps in the direction of the steepest decrease of <span class="math notranslate nohighlight">\(J\)</span>. We can work out the partial derivative directly, supposing there were only one trianing example:</p>
<p><span class="math">\begin{align}
\frac{\partial}{\partial \theta_j} J(\theta)
&= \frac{\partial}{\partial \theta_j} \frac{1}{2} \left(h_\theta(x) - y\right)^2 \\
&= \left(h_\theta(x) - y\right) \cdot \frac{\partial}{\partial \theta_j} \left(h_\theta(x) - y\right) \\
&=\left(h_\theta(x) - y\right) \cdot \frac{\partial}{\partial \theta_j} \left(\sum_{i=0}^n \theta_i x_i - y\right) \\
&= \left(h_\theta(x) - y\right) x_j
\end{align}</span></p>
<p>giving the update rule</p>
<div class="math notranslate nohighlight">
\[\theta_{j} := \theta_j + \alpha\left(y^{(i)} - h_\theta(x^{(i)}\right)x_j^{(i)}.\]</div>
<p>The interpretation here is that we compute the direction of the gradient <span class="math notranslate nohighlight">\(\nabla J = \frac{\partial J}{\partial \theta_j}\)</span> and walk a step size <span class="math notranslate nohighlight">\(\alpha\)</span> in the negative gradient direction (direction of steepest decrease). This is called the least mean squares (LMS) update rule. However, in the case of more than one training data point, we simply use the sum as if we were taking the derivative with respect to <span class="math notranslate nohighlight">\(J(\theta)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\theta_j := \theta_j + \alpha \frac{1}{m}\sum_{i=1}^m \left(y^{(i)} - h_\theta\left(x^{(i)}\right)\right) x_j^{(i)} \qquad \text{for every } j\]</div>
<p>Since this uses every training example at every step, this is called <strong>batch gradient descent</strong>. Because <span class="math notranslate nohighlight">\(J(\theta)\)</span> is a convex quadratic function, this algorithm should always converge, given an <span class="math notranslate nohighlight">\(\alpha\)</span> not too large.</p>
<p>Here we write an implementation of batch gradient descent.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">batch_gradient_descent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">θ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span>
    <span class="n">h</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">θ</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">θ</span><span class="p">)</span>

    <span class="n">θ_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">θ</span><span class="p">),</span> <span class="n">iterations</span><span class="p">))</span>
    <span class="n">cost_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">iterations</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">θ</span> <span class="o">+=</span>  <span class="n">α</span> <span class="o">*</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">h</span><span class="p">(</span><span class="n">θ</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
        <span class="n">cost_history</span><span class="p">[</span><span class="n">iteration</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span> <span class="n">h</span><span class="p">(</span><span class="n">θ</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">θ_history</span><span class="p">[:,</span><span class="n">iteration</span><span class="p">]</span> <span class="o">=</span> <span class="n">θ</span>
    <span class="k">return</span> <span class="n">θ</span><span class="p">,</span> <span class="n">θ_history</span><span class="p">,</span> <span class="n">cost_history</span>

</pre></div>
</div>
</div>
<p>We create a noisy dataset to test our algorithm.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">d</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">x</span><span class="p">[:,</span> <span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<p>We use our implementation to find the line of best fit, and plot the convergence history.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># run GD</span>
<span class="n">θ</span><span class="p">,</span> <span class="n">hist</span><span class="p">,</span> <span class="n">cost</span> <span class="o">=</span> <span class="n">batch_gradient_descent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="c1"># generate best fit line</span>
<span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">θ</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">x_test</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># compare to least squares</span>
<span class="n">lst</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">blst</span><span class="p">,</span> <span class="n">alst</span> <span class="o">=</span> <span class="n">lst</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y_lst</span> <span class="o">=</span> <span class="n">alst</span><span class="o">*</span><span class="n">x_test</span> <span class="o">+</span> <span class="n">blst</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/home/foucartc/anaconda3/envs/NMSMExp_env/lib/python3.6/site-packages/ipykernel/__main__.py:2: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.
To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.
  from ipykernel import kernelapp as app
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;best fit line, batch gradient descent&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_lst</span><span class="p">,</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;least sq&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
           <span class="n">label</span><span class="o">=</span><span class="s1">&#39;batch grad desc&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>


<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$ convergence history&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">hist</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="s1">&#39;-o&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cost</span><span class="p">)),</span> <span class="n">cost</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;cost function $J(\theta)$ vs. iterations&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_linear_regression_27_0.png" src="../_images/tutorial_linear_regression_27_0.png" />
</div>
</div>
</div>
<div class="section" id="Stochastic-gradient-descent">
<h2>3.4. Stochastic gradient descent<a class="headerlink" href="#Stochastic-gradient-descent" title="Permalink to this headline">¶</a></h2>
<p>The problem with batch gradient descent is that if the dataset is large (<span class="math notranslate nohighlight">\(m\)</span> large), we have to compute the sum over the entire dataset before we update <span class="math notranslate nohighlight">\(\theta_j\)</span>. A cheaper alternative that tends to work very well in practice is <strong>stochastic gradient descent</strong>, which updates <span class="math notranslate nohighlight">\(\theta_j\)</span> at every training set example. In practice, stochastic gradient descent tends to outperform batch gradient descent.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">stochastic_gradient_descent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">θ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span>
    <span class="n">h</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">θ</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">θ</span><span class="p">)</span>

    <span class="n">θ_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">θ</span><span class="p">),</span> <span class="n">iterations</span><span class="p">))</span>
    <span class="n">cost_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">iterations</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
            <span class="n">θ</span> <span class="o">+=</span>  <span class="n">α</span> <span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">h</span><span class="p">(</span><span class="n">θ</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,:]))</span>
        <span class="n">cost_history</span><span class="p">[</span><span class="n">iteration</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span> <span class="n">h</span><span class="p">(</span><span class="n">θ</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">θ_history</span><span class="p">[:,</span><span class="n">iteration</span><span class="p">]</span> <span class="o">=</span> <span class="n">θ</span>
    <span class="k">return</span> <span class="n">θ</span><span class="p">,</span> <span class="n">θ_history</span><span class="p">,</span> <span class="n">cost_history</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># run GD</span>
<span class="n">θ</span><span class="p">,</span> <span class="n">hist</span><span class="p">,</span> <span class="n">cost</span> <span class="o">=</span> <span class="n">stochastic_gradient_descent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

<span class="c1"># generate best fit line</span>
<span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">θ</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">x_test</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;best fit line, stochastic gradient descent&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_lst</span><span class="p">,</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;least sq&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;stoch grd desc&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>


<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$ convergence history&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">hist</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="s1">&#39;-o&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cost</span><span class="p">)),</span> <span class="n">cost</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;cost function $J(\theta)$ vs. iterations&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_linear_regression_32_0.png" src="../_images/tutorial_linear_regression_32_0.png" />
</div>
</div>
<p>Note that SGD may be less accurate than batch gradient descent, and may oscillate around the local minimum. However, when the training set is very large, it is often preferred over batch gradient descent.</p>
<div class="section" id="a-remark-on-“learning-rate”-\alpha">
<h3>3.4.1. a remark on “learning rate” <span class="math notranslate nohighlight">\(\alpha\)</span><a class="headerlink" href="#a-remark-on-“learning-rate”-\alpha" title="Permalink to this headline">¶</a></h3>
<p>Returning to the interpretation of <span class="math notranslate nohighlight">\(\alpha\)</span> as the step size we take in the direction of the negative gradient <span class="math notranslate nohighlight">\(-\nabla J(\theta)\)</span>, a natural question is what step size should be used.</p>
<p>In very simple cases, such as gradient descent for solving a system of linear equations (and hence a minimization over a convex, quadratic objective function), the optimal step size can be computed analytically. If the current iterate is <span class="math notranslate nohighlight">\(\theta^{(i)}\)</span>, then we take <span class="math notranslate nohighlight">\(\alpha\)</span> as the distance that minmizes <span class="math notranslate nohighlight">\(J(\theta)\)</span> along the search line <span class="math notranslate nohighlight">\(\theta^{(i)} + \alpha v\)</span> where <span class="math notranslate nohighlight">\(v\)</span> is the direction of steepest descent; that is,</p>
<p><span class="math">\begin{equation}
\alpha = \arg \min_{\gamma} J(\theta^{(i)} + \gamma v, \qquad v = -\nabla J(\theta)
\end{equation}</span></p>
<p>which we find by taking the derivative of <span class="math notranslate nohighlight">\(J\)</span> along the search direction with respect to <span class="math notranslate nohighlight">\(\gamma\)</span> and set it equal to zero:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{d\gamma} J(\theta^{(i)} + \gamma v) = 0.\]</div>
<p>However, for more general objective functions, computing the derivative with respect to <span class="math notranslate nohighlight">\(\gamma\)</span> may be expensive, and potentially has no guarantee of a minimum. So we can only use this step size in special cases.</p>
<p>A more general heuristic to find a good step size is using the so-called <strong>backtracking line search</strong>. In this approach at each step, we choose a <span class="math notranslate nohighlight">\(\gamma_0\)</span> which is usually more ambitious than the previous value of <span class="math notranslate nohighlight">\(\gamma\)</span>. Then we check to see if the point <span class="math notranslate nohighlight">\(\theta^{(i)} + \gamma_0 v\)</span> is good, e.g., with the Armijo-Goldstein condition</p>
<div class="math notranslate nohighlight">
\[J(\theta^{(i)} + \gamma_0 v) \leq J(\theta^{(i)}) - c\gamma_0 \,||\nabla J(\theta^{(i)}) ||_2^2.\]</div>
<p>If it satisfies, just take the step. If it doesn’t, try the test again with a smaller <span class="math notranslate nohighlight">\(\gamma_1 =\gamma_0/2\)</span> and continue to divide by two until the criterion is satisfied. Then take the step. This is generally much cheaper than doing the exact line search.</p>
</div>
</div>
<div class="section" id="Probabilistic-interpretation">
<h2>3.5. Probabilistic interpretation<a class="headerlink" href="#Probabilistic-interpretation" title="Permalink to this headline">¶</a></h2>
<p>Least squares as we’ve seen it can be boiled down to minimizing the cost function <span class="math">\begin{equation}
J(\theta) = \frac{1}{2} \sum_{i=1}^{m} \left(y^{(i)} - \theta^T x^{(i)}\right)^2
\end{equation}</span></p>
<p>Which lends itself to the question of: why this cost function? Why not minimize a different cost function. For example, we could have easily chose to minimize the cost functions</p>
<p><span class="math">\begin{align}
J_1(\theta) &= \frac{1}{2} \sum_{i=1}^{m} \left|y^{(i)} - \theta^T
x^{(i)}\right| \\
J_3(\theta) &= \frac{1}{2} \sum_{i=1}^{m} \left(y^{(i)} - \theta^T
x^{(i)}\right)^3.
\end{align}</span></p>
<p>However, there is a nice probabilistic argument that suggests least squares is the way to go. Suppose that all the outputs were generated by our model, but with some random Gaussian noise.</p>
<p>This noise <span class="math notranslate nohighlight">\(\epsilon^{(i)}\)</span> could represent features that we missed with our model, or noisy observation—but we presume that it’s normally distributed with mean zero and variance <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>, i.e., the probability mass function over the noise <span class="math notranslate nohighlight">\(\epsilon\)</span></p>
<p><span class="math">\begin{equation}
p_{\epsilon}\left(\epsilon^{(i)}\right) = \frac{1}{\sigma\sqrt{2\pi}}
\exp\left(-\frac{\left(\epsilon^{(i)}\right)^2}{2 \sigma^2} \right)
\end{equation}</span></p>
<p>and that the noise is i.i.d..</p>
<p>These aren’t terrible assumptions because the central limit theorem suggests that a Gaussian representation is a good one for the sum of many i.i.d. random variables, and the i.i.d. assumption is usually a good one because we presume the unknown factors are fair (they’ll sometimes overestimate, and sometimes underestimate the true value but won’t be biased) and that they’re independent of one another.</p>
<p>If all this is true, then we can write down the probability of a given observation <span class="math notranslate nohighlight">\(y^{(i)}\)</span> given the features <span class="math notranslate nohighlight">\(x^{(i)}\)</span> is the conditional probability</p>
<p><span class="math">\begin{equation}
p\left(y^{(i)}|x^{(i)};\theta\right) = \frac{1}{\sigma\sqrt{2\pi}}
\exp\left(-\frac{\left(y^{(i)}- \theta^T x^{(i)}\right)^{2}}{2 \sigma^2}
\right).
\end{equation}</span></p>
<p>This makes sense, since the error is the only random part, so we would expect the observation to be normally distributed with mean <span class="math notranslate nohighlight">\(\theta^t x^{(i)}\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, i.e., <span class="math notranslate nohighlight">\(p\left(y^{(i)}|x^{(i)}; \theta\right)\sim \mathcal{N}\left(\theta^T x^{(i)}, \sigma^2\right)\)</span>—this notation expresses that we are not conditioning on <span class="math notranslate nohighlight">\(\theta\)</span> since <span class="math notranslate nohighlight">\(\theta\)</span> is not a random variable, it’s our model (this a frequentist way of viewing things).</p>
<p>We could consider the distribution over all the observations <span class="math notranslate nohighlight">\(y\)</span> (a vector), given a set of inputs <span class="math notranslate nohighlight">\(X\)</span> (sometimes called the design matrix) for a fixed value of <span class="math notranslate nohighlight">\(\theta\)</span>; we would write it as <span class="math notranslate nohighlight">\(p\left(y|X;\theta\right)\)</span>. If we wanted to view this distribution as a function of <span class="math notranslate nohighlight">\(\theta\)</span> instead, we could treat <span class="math notranslate nohighlight">\(\theta\)</span> as a parameter. We call this the likelihood function</p>
<p><span class="math">\begin{equation}
L(\theta) = L(\theta; X,y) = p(y| X;\theta).
\end{equation}</span></p>
<p>But since all the randomness comes from the noise, and the noises <span class="math notranslate nohighlight">\(\epsilon^{(i)}\)</span> are independent of one another, we can explicitly write out the likelihood as</p>
<p><span class="math">\begin{align}
L(\theta) &= \prod_{i=1}^m p\left(y^{(i)}|x^{(i)};\theta\right) \\
&= \prod_{i=1}^m \frac{1}{\sigma\sqrt{2\pi}}
\exp\left(-\frac{\left(y^{(i)}- \theta^T x^{(i)}\right)^{2}}{2 \sigma^2}
\right).
\end{align}</span></p>
<p>If we were interested in finding the best linear model <span class="math notranslate nohighlight">\(\theta\)</span> to fit the observations <span class="math notranslate nohighlight">\(y\)</span> given the data <span class="math notranslate nohighlight">\(X\)</span>, we would want to maximize the likelihood over <span class="math notranslate nohighlight">\(\theta\)</span> (maximum likelihood estimate). We are searching for the <span class="math notranslate nohighlight">\(\theta\)</span> which maximizes the probability of making the observations given the data.</p>
<p>However, for numerical reasons and ease of derivations, we maximize the log-likelihood</p>
<p><span class="math">\begin{equation}
\ell(\theta) = \log L(\theta),
\end{equation}</span></p>
<p>rather than the likelihood itself. Writing out the algebra, we wish to maximize</p>
<p><span class="math">\begin{align}
\ell(\theta) &= \log \prod_{i=1}^m \frac{1}{\sigma\sqrt{2\pi}}
\exp\left(-\frac{\left(y^{(i)}- \theta^T x^{(i)}\right)^{2}}{2 \sigma^2}
\right)\\
&= \sum_{i=1}^{m} \left[
\log
\left(\frac{1}{\sigma\sqrt{2\pi}}\right)
+ \log\exp\left(-\frac{\left(y^{(i)}- \theta^T x^{(i)}\right)^{2}}{2 \sigma^2}\right)
\right] \\
&= \underbrace{m\log
\left(\frac{1}{\sigma\sqrt{2\pi}}\right)}_{\text{constant, doesn't affect maximization}}
- \frac{1}{\sigma^2} \cdot
\underbrace{
\frac{1}{2}\left(-\frac{\left(y^{(i)}- \theta^T x^{(i)}\right)^{2}}{2
\sigma^2}\right)}_{J(\theta)}
\end{align}</span></p>
<p>So maximizing the log-likelihood reduces to minimizing the original cost function <span class="math notranslate nohighlight">\(J(\theta)\)</span> from least squares. So least squares corresponds to a maximum likelihood estimate of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
</div>
<div class="section" id="Locally-weighted-linear-regression">
<h2>3.6. Locally weighted linear regression<a class="headerlink" href="#Locally-weighted-linear-regression" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="Linear-regression-using-sckikit-learn">
<h2>3.7. Linear regression using <code class="docutils literal notranslate"><span class="pre">sckikit-learn</span></code><a class="headerlink" href="#Linear-regression-using-sckikit-learn" title="Permalink to this headline">¶</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="newton_methods.html" class="btn btn-neutral float-right" title="4. Newton methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="PRML_ch3_lin_reg.html" class="btn btn-neutral float-left" title="2. Linear models for regression" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Corbin Foucart

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>