

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>1. Introduction &mdash; NMSMExp 0.2 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"equationNumbers": {"autoNumber": "AMS", "useLabelIds": true}}, "tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Linear models for regression" href="PRML_ch3_lin_reg.html" />
    <link rel="prev" title="3. Introduction to single parameter models" href="bayesian_inference_ch1_single_parameter_models.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> NMSMExp
          

          
          </a>

          
            
            
              <div class="version">
                0.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Linear Algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#probability-and-statistics">Probability and Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#convex-optimization">Convex Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#bayesian-inference">Bayesian Inference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html#machine-learning">Machine Learning</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">1. Introduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Polynomial-curve-fitting">1.1. Polynomial curve fitting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Solving-the-minimization-problem">1.1.1. Solving the minimization problem</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Model-selection-and-overfitting">1.2. Model selection and overfitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Regularization">1.3. Regularization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Probabilistic-viewpoints">1.4. Probabilistic viewpoints</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Unknown-error-variance">1.4.1. Unknown error variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Bayesian-curve-fitting">1.4.2. Bayesian curve fitting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Model-selection">1.5. Model selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Appendix">1.6. Appendix</a></li>
<li class="toctree-l3"><a class="reference internal" href="#References">1.7. References</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="PRML_ch3_lin_reg.html">2. Linear models for regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_regression.html">3. Linear regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="newton_methods.html">4. Newton methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="logistic_regression.html">5. Classification and logistic regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="perceptron.html">6. Random linear classifiers</a></li>
<li class="toctree-l2"><a class="reference internal" href="perceptron.html#Perceptrons">7. Perceptrons</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#gaussian-processes">Gaussian Processes</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NMSMExp</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Linear Algebra</a> &raquo;</li>
        
      <li>1. Introduction</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorial/PRML_ch1_intro.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># constants</span>
<span class="n">π</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>
</pre></div>
</div>
</div>
<div class="section" id="Introduction">
<h1>1. Introduction<a class="headerlink" href="#Introduction" title="Permalink to this headline">¶</a></h1>
<p>This notebook will introduce some high-level ideas in machine learning through the lens of the example of polynomial curve fitting. We will see:</p>
<ul class="simple">
<li><p>Machine learning models are trained on “training data” and can be evaluated on unseen “test data”</p></li>
<li><p>Fitting data to a polynomial curve amounts to solving a linear system</p></li>
<li><p>Models can be fit too well to the training data (overfitting) don’t do well on other data</p></li>
<li><p>We can help alleviate overfitting with a process called “regularization”</p></li>
<li><p>We can attempt to quantify model uncertainty in machine learning models by incorporating Bayesian ideas</p></li>
</ul>
<div class="section" id="Polynomial-curve-fitting">
<h2>1.1. Polynomial curve fitting<a class="headerlink" href="#Polynomial-curve-fitting" title="Permalink to this headline">¶</a></h2>
<p>Take a training set comprising <span class="math notranslate nohighlight">\(N\)</span> observations at <span class="math notranslate nohighlight">\(x= (x_1,\ldots,x_n)\)</span> with observed values <span class="math notranslate nohighlight">\(t=(t_1,\ldots,t_n)\)</span>. In many real data sets, there is an underlying regularity to be learned, but individual observations are corrupted by noise. This might be due to true stochasticity introduced by random processes, or there could be sources of variability which are unobserved.</p>
<p>The goal is for a new input <span class="math notranslate nohighlight">\(\widehat{x}\)</span>, to predict the target variable <span class="math notranslate nohighlight">\(\widehat{t}\)</span>. We use a simple curve fitting model with the function <span class="math notranslate nohighlight">\(y\colon \mathbf{R}^{}\times \mathbf{R}^{m+1}\to \mathbf{R}^{}\)</span></p>
<p><span class="math">\begin{equation}
y(x,w) = w_0 + w_1 x + \ldots w_M x^M = \sum_{i=0}^{M} w_i x^i
\end{equation}</span></p>
<p>This is called a “linear model” because although <span class="math notranslate nohighlight">\(y(x,w)\)</span> is nonlinear in <span class="math notranslate nohighlight">\(x\)</span>, it is linear in <span class="math notranslate nohighlight">\(w\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">π</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">σ</span> <span class="o">=</span> <span class="mf">0.3</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">N</span><span class="p">)</span>
<span class="n">ϵ</span> <span class="o">=</span> <span class="n">σ</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">ϵ</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">xx</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;observations&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_PRML_ch1_intro_6_0.png" src="../_images/tutorial_PRML_ch1_intro_6_0.png" />
</div>
</div>
<p>We will determine <span class="math notranslate nohighlight">\(w\)</span> by minimizing an error function that measures the misfit between the training set and the function <span class="math notranslate nohighlight">\(y(x,w)\)</span>. A simple choice for error functions is the sum of squared errors.</p>
<p><span class="math">\begin{equation}
E(w) = \frac{1}{2} \sum_{n=1}^{N} \left(y(x_n, w) - t_n\right)^2
\end{equation}</span></p>
<p>We want to minimize this error function over the available training data.</p>
<div class="section" id="Solving-the-minimization-problem">
<h3>1.1.1. Solving the minimization problem<a class="headerlink" href="#Solving-the-minimization-problem" title="Permalink to this headline">¶</a></h3>
<p>We seek <span class="math notranslate nohighlight">\(w\)</span> such that <span class="math notranslate nohighlight">\(\nabla_w E(w) = 0\)</span>. Each component of the gradient should vanish.</p>
<p><span class="math">\begin{align}
\frac{\partial }{\partial w_i}E(w)
&=
\frac{\partial }{\partial w_i} \frac{1}{2}\sum_{n=1}^{N} \left(y(x_n,w) - t_n\right)^2 \\
&= \sum_{n=1}^{N} \left(y(x_n,w) -t_n\right) \frac{\partial }{\partial w_i}(y(x_n,w) - t_n)\\
&= \sum_{n=1}^{N} \left(y(x_n,w) -t_n\right)x_n^i
\end{align}</span></p>
<p>since</p>
<p><span class="math">\begin{equation}
\frac{\partial }{\partial w_i}(y(x_n,w) - t_n)
=\frac{\partial }{\partial w_i}(w_0 + \ldots + w_M x^M - t_n)
= x^i
\end{equation}</span></p>
<p>The condition that the <span class="math notranslate nohighlight">\(i^{th}\)</span> component vanishes is</p>
<p><span class="math">\begin{equation}
\sum_{n=1}^{N} w_0 x_n^i + \ldots w_Mx_n^{i+M} - t_n x_n^i = 0
\end{equation}</span></p>
<p>which can be written as the linear system <span class="math notranslate nohighlight">\(Aw = b\)</span> where</p>
<p><span class="math">\begin{equation}
A_{ij} = \sum_{n=1}^{N} x_n^{i+j}, \qquad b_i = \sum_{n=1}^{N} t_n x^i.
\end{equation}</span></p>
<p>The function below computes weights for a given training set and polynomial order.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">polyfit_weights</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="c1"># increment M to account for 1x1 matrix at p=0</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">M</span><span class="o">+</span><span class="mi">1</span>
    <span class="n">A</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">M</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
            <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="n">j</span><span class="p">))</span>
            <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="n">i</span><span class="p">))</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">M</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">]):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">polyfit_weights</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">xx</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$M=</span><span class="si">{}</span><span class="s2">$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">M</span><span class="p">));</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_PRML_ch1_intro_12_0.png" src="../_images/tutorial_PRML_ch1_intro_12_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="Model-selection-and-overfitting">
<h2>1.2. Model selection and overfitting<a class="headerlink" href="#Model-selection-and-overfitting" title="Permalink to this headline">¶</a></h2>
<p>Increasing the model order to <span class="math notranslate nohighlight">\(M=9\)</span> results in no training error and the polynomial passes through every training point. However, as a result, the function oscillates wildly and poorly represents the underlying data. This behavior is called <em>overfitting</em>.</p>
<p>To better assess generalization, we can segment all data into a training dataset and a test dataset and select a model that generalizes well to the test data.</p>
<p>The root mean squared (RMS) error is defined as</p>
<p><span class="math">\begin{equation}
E_{\text{RMS}} = \sqrt{2 E(w)/N}
\end{equation}</span></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">generate_dataset</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">σ</span><span class="o">=</span><span class="mf">0.3</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">N</span><span class="p">)</span>
    <span class="n">ϵ</span> <span class="o">=</span> <span class="n">σ</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">ϵ</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y_xw</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>
<span class="n">E</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">E_RMS</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">E</span><span class="p">,</span><span class="n">N</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">E</span><span class="o">/</span><span class="n">N</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># training data</span>
<span class="n">N_train</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span> <span class="o">=</span> <span class="n">generate_dataset</span><span class="p">(</span><span class="n">N_train</span><span class="p">)</span>

<span class="c1"># testing data</span>
<span class="n">N_test</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N_test</span><span class="p">)</span>
<span class="n">ϵ</span> <span class="o">=</span> <span class="n">σ</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N_test</span><span class="p">)</span>
<span class="n">t_test</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span> <span class="o">+</span> <span class="n">ϵ</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">RMS_errs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;test&#39;</span><span class="p">:[],</span> <span class="s1">&#39;train&#39;</span><span class="p">:[]}</span>
<span class="n">ws</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">polyfit_weights</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
    <span class="n">fitted</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">training_error</span> <span class="o">=</span> <span class="n">E</span><span class="p">(</span><span class="n">fitted</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">)</span>
    <span class="n">test_error</span> <span class="o">=</span> <span class="n">E</span><span class="p">(</span><span class="n">fitted</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>
    <span class="n">RMS_errs</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">E_RMS</span><span class="p">(</span><span class="n">training_error</span><span class="p">,</span> <span class="n">N_train</span><span class="p">))</span>
    <span class="n">RMS_errs</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">E_RMS</span><span class="p">(</span><span class="n">test_error</span><span class="p">,</span> <span class="n">N_test</span><span class="p">))</span>
    <span class="n">ws</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">RMS_errs</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">RMS_errs</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;RMS error vs model order $M$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_PRML_ch1_intro_19_0.png" src="../_images/tutorial_PRML_ch1_intro_19_0.png" />
</div>
</div>
<p>There are diminishing returns past an order <span class="math notranslate nohighlight">\(M=3\)</span> and bad overfitting at <span class="math notranslate nohighlight">\(M=9\)</span>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">ws</span><span class="p">[</span><span class="n">m</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">*</span><span class="n">m</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;absolute coefficient values $|w|$ vs $M$&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_PRML_ch1_intro_21_0.png" src="../_images/tutorial_PRML_ch1_intro_21_0.png" />
</div>
</div>
<p>A numerical interpretation is that the coefficients are growing very large in order to exactly fit the training set at <span class="math notranslate nohighlight">\(M=9\)</span>.</p>
</div>
<div class="section" id="Regularization">
<h2>1.3. Regularization<a class="headerlink" href="#Regularization" title="Permalink to this headline">¶</a></h2>
<p>To stop the coefficients from growing too large, <em>regularization</em> adds a penalty term to the error function. One choice is the modified error function</p>
<p><span class="math">\begin{equation}
\tilde{E}(w) = \frac{1}{2} \sum_{n=1}^{N} \left(y(x_n, w) - t_n\right)^2 +
\frac{\lambda}{2} \left\Vert w\right\Vert_{2}^{2}
\label{eq:ridge_reg}
\end{equation}</span></p>
<p>The parameter <span class="math notranslate nohighlight">\(\lambda\)</span> determines the relative importance of the regularization term and the sum-of-squares error term. The coefficient <span class="math notranslate nohighlight">\(w_0\)</span> is often ommitted from regularization because it represents an offset which corresponds to choice of origin. The regularization choice in (<span class="math">\ref{eq:ridge_reg}</span>) is called <em>ridge regression</em>.</p>
<p>Using our previous result, we can modify our condition that the gradient with respect to the weights vanishes.</p>
<p>since</p>
<p><span class="math">\begin{equation}
\frac{\partial }{\partial w_i} \frac{\lambda}{2} \left\lVert w\right\rVert_2^2
=  \frac{\lambda}{2} \frac{\partial }{\partial w_i} \sum_{i=1}^{M} w_i^2
= \lambda w_i,
\end{equation}</span></p>
<p>The modified gradient is</p>
<p><span class="math">\begin{equation}
\frac{\partial \tilde{E}}{\partial w_i}
= \sum_{n=1}^{N} \left(y(x_n, w) - t_n\right) x_n^i
+\lambda w_i
\end{equation}</span></p>
<p>From which it is straightforward to work out that the only modification to the linear system is the addition of a <span class="math notranslate nohighlight">\(\lambda\)</span> term to the diagonal.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">regularized_polyfit_weights</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">λ</span><span class="p">):</span>
    <span class="c1"># increment M to account for 1x1 matrix at p=0</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">M</span><span class="o">+</span><span class="mi">1</span>
    <span class="n">A</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">M</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
            <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="n">j</span><span class="p">))</span>
            <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="n">i</span><span class="p">))</span>
        <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">λ</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># training data</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">N_train</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span> <span class="o">=</span> <span class="n">generate_dataset</span><span class="p">(</span><span class="n">N_train</span><span class="p">)</span>

<span class="c1"># compute regularized and unregularized weights</span>
<span class="n">w_unreg</span> <span class="o">=</span> <span class="n">polyfit_weights</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
<span class="n">w_reg</span> <span class="o">=</span> <span class="n">regularized_polyfit_weights</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">λ</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">17</span><span class="p">))</span>

<span class="c1"># plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="n">ax</span><span class="p">:</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">xx</span><span class="p">))</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">w_unreg</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">);</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;no regularization&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">w_reg</span><span class="p">));</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\ln\,\lambda = -17$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_PRML_ch1_intro_26_0.png" src="../_images/tutorial_PRML_ch1_intro_26_0.png" />
</div>
</div>
</div>
<div class="section" id="Probabilistic-viewpoints">
<h2>1.4. Probabilistic viewpoints<a class="headerlink" href="#Probabilistic-viewpoints" title="Permalink to this headline">¶</a></h2>
<p>If we assume that, given an input value <span class="math notranslate nohighlight">\(x_0\)</span>, the target value will be normally distributed around the predicted value <span class="math notranslate nohighlight">\(y(x_0,w)\)</span>, then the probability of making an observation <span class="math notranslate nohighlight">\(t_0\)</span> is described by a Gaussian probability density function with mean <span class="math notranslate nohighlight">\(y(x_0,w)\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> (which we assume is known).</p>
<p><span class="math">\begin{equation*}
P(t_0|x_0, w, \sigma^2) = \mathcal{N}\left(t \mid y(x_0,w), \beta^{-1}\right),
\end{equation*}</span></p>
<p>where we have written the distribution in terms of the precision <span class="math notranslate nohighlight">\(\beta \equiv 1/\sigma^2\)</span>. Observations are presumed to be independent, so the likelihood of the set of target observations <span class="math notranslate nohighlight">\(t\)</span> given the inputs <span class="math notranslate nohighlight">\(x\)</span> is</p>
<p><span class="math">\begin{equation*}
P(t|x,w,\beta) = \prod_{n=1}^N \frac{1}{(2\pi\sigma^2)^{1/2}} \exp\left(-\frac{1}{2\sigma^2} (y(x_n,w) - t_n)^2\right)
\end{equation*}</span></p>
<p>From a prediction point of view, we seek the <span class="math notranslate nohighlight">\(w\)</span> which maximizes the likelihood given the inputs <span class="math notranslate nohighlight">\(x\)</span>. This is equivalent to maximizing the log-likelihood, which we can write as</p>
<p><span class="math">\begin{align*}
\log P(t|x,w,\beta) &= \log \prod_{n=1}^N \frac{1}{(2\pi\sigma^2)^{1/2}} \exp\left(-\frac{1}{2\sigma^2} (y(x_n,w) - t_n)^2\right)\\
&= \sum_{n=1}^N \log\frac{1}{(2\pi)^{1/2}} + \log \left(\frac{1}{\sigma^2}\right)^{1/2}  +  \left(-\frac{1}{2\sigma^2} (y(x_n,w) - t_n)^2\right)\\
&= -\frac{N}{2} \log(2\pi) + \frac{N}{2} \log\beta - \sum_{n=1}^{N} \frac{\beta}{2} \left(y(x_n,w) - t_n\right)^2
\end{align*}</span></p>
<p>The first two terms in the log-likelihood don’t depend on <span class="math notranslate nohighlight">\(w\)</span> and can be dropped, since they don’t affect the optimal value. Finally, instead of maximizing the log-likelihood, we can minimize the negative log-likelihood, which amounts to solving the equivalent unconstrained optimization problem</p>
<p><span class="math">\begin{align*}
\text{minimize}  \; &  f_0(w) = \sum_{n=1}^{N}  \left(y(x_n,w) - t_n\right)^2     &     \\
\end{align*}</span></p>
<p>Thus minimizing the sum-of-squares error function is equivalent to the maximum likelihood estimate of <span class="math notranslate nohighlight">\(w\)</span> under the assumption of Gaussian noise.</p>
<div class="section" id="Unknown-error-variance">
<h3>1.4.1. Unknown error variance<a class="headerlink" href="#Unknown-error-variance" title="Permalink to this headline">¶</a></h3>
<p>We previously supposed the variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> of the error distribution between predicted value <span class="math notranslate nohighlight">\(y(x_0,w\)</span> and target value <span class="math notranslate nohighlight">\(t_0\)</span> was known, but often, the variance is not known. In this case, we can find a maximum likelihood estimate of <span class="math notranslate nohighlight">\(\beta\)</span> as well by treating it as an unknown and maximizing the log-likelihood with respect to <span class="math notranslate nohighlight">\(\beta\)</span></p>
<p><span class="math">\begin{align*}
\text{maximize}  \; & f_0(\beta) = \frac{N}{2} \log\beta - \sum_{n=1}^{N} \frac{\beta}{2} \left(y(x_n,w) - t_n\right)^2    &     \\
\end{align*}</span></p>
<p>by taking the partial derivative with respect to <span class="math notranslate nohighlight">\(\beta\)</span>,</p>
<p><span class="math">\begin{equation*}
\frac{\partial f_0}{\partial \beta} = \frac{N}{2\beta} - \frac{1}{2} \sum_{n=1}^{N} \left(y(x_n,w) - t_n\right)^2 = 0
\end{equation*}</span></p>
<p>yielding the maximum-likelihood estimate for the variance</p>
<p><span class="math">\begin{equation*}
\sigma^2_{\text{MLE}} =  \frac{1}{\beta_{\text{MLE}}} = \frac{1}{N} \sum_{n=1}^{N} \left(y(x_n,w) - t_n\right)^2.
\end{equation*}</span></p>
<p>We can use the error in the regularization example above to estimate the variance.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">σ2_MLE</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">N_train</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">y</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">w_reg</span><span class="p">)</span> <span class="o">-</span> <span class="n">t_train</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
<span class="n">σ_MLE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">σ2_MLE</span><span class="p">)</span>
<span class="n">σ_MLE</span><span class="p">,</span> <span class="n">σ</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>(0.14881932780263604, 0.3)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># plot</span>
<span class="n">xx</span> <span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>
<span class="n">yxw</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">w_reg</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="n">ax</span><span class="p">:</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">xx</span><span class="p">))</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yxw</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yxw</span><span class="o">-</span><span class="n">σ_MLE</span><span class="p">,</span> <span class="n">yxw</span><span class="o">+</span><span class="n">σ_MLE</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yxw</span><span class="o">-</span><span class="n">σ</span><span class="p">,</span> <span class="n">yxw</span><span class="o">+</span><span class="n">σ</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\sigma_</span><span class="si">{MLE}</span><span class="s1">$&#39;</span><span class="p">);</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\sigma$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_PRML_ch1_intro_33_0.png" src="../_images/tutorial_PRML_ch1_intro_33_0.png" />
</div>
</div>
<p>We see that the esimate of the variance is smaller than the actual variance. That’s because we chose a large enough variance in generating the data such that the curve fitting is “fit” better to the data than the actual underlying function, since we chose to minimize the sum-of-squares error to generate the weights in the first place!</p>
<p>This would improve with additional training data.</p>
</div>
<div class="section" id="Bayesian-curve-fitting">
<h3>1.4.2. Bayesian curve fitting<a class="headerlink" href="#Bayesian-curve-fitting" title="Permalink to this headline">¶</a></h3>
<p>In the previous section, we made point estimates of <span class="math notranslate nohighlight">\(w\)</span> as well as <span class="math notranslate nohighlight">\(\beta\)</span>. A step towards Bayesian treatment would be to introduce a prior distribution over the weights <span class="math notranslate nohighlight">\(w\)</span>.</p>
<p><strong>Introducing a prior distribution over</strong> <span class="math notranslate nohighlight">\(w\)</span></p>
<p>We introduce a prior over the weights <span class="math notranslate nohighlight">\(w\)</span>. For simplicity, we choose a multivariate Gaussian prior on the weights with zero mean and precision <span class="math notranslate nohighlight">\(\alpha\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\Sigma = \alpha^{-1}I\)</span>. We assume that the precisions <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are known.</p>
<p><span class="math">\begin{align*}
P(w|\alpha) &= \mathcal{N}\left(w|0, \alpha^{-1}I\right) \\
&= \frac{1}{(2\pi)^{(M+1)/2}|\alpha^{-1}I|} \exp\left(-\frac{1}{2}(w-0)^T\,\Sigma^{-1}(w-0)\right) \\
&= \left(\frac{\alpha}{2\pi} \right)^{\frac{M+1}{2}} \exp\left(-\frac{\alpha}{2} w^T w\right)
\end{align*}</span></p>
<p>where we have used</p>
<p><span class="math">\begin{equation*}
|\alpha^{-1}I|^{-1} = \left(\alpha^{-1}\right)^{-(M+1)/2} = \alpha^{(M+1)/2}.
\end{equation*}</span></p>
<p>The posterior distribution over <span class="math notranslate nohighlight">\(w\)</span> is</p>
<p><span class="math">\begin{equation*}
P(w|x,t,\alpha, \beta) \propto P(t|x, w,\beta) P(w|\alpha)
\end{equation*}</span></p>
<p>with log-likelihood</p>
<p><span class="math">\begin{align*}
\log P(w|x,t,\alpha,\beta) &\propto \log P(t|x, w,\beta) +  \log P(w|\alpha) \\
&= - \frac{\beta}{2} \sum_{n=1}^{N} \left(y(x_n,w) - t_n\right)^2
+ \frac{N}{2} \log \beta
- \frac{N}{2} \log(2\pi) \\
&\qquad + \left(\frac{M+1}{2}\right) \log \left(\frac{\alpha}{2\pi} \right)
- \frac{\alpha}{2} w^T w
\end{align*}</span></p>
<p>For the purposes of maximizing the log-likelihood with respect to the weights <span class="math notranslate nohighlight">\(w\)</span>, we need only consider terms which depend on <span class="math notranslate nohighlight">\(w\)</span>, yielding the equivalent minimization problem</p>
<p><span class="math">\begin{align}
\text{minimize}  \; & f_0(x) = \frac{1}{2} \sum_{n=1}^{N} \left(y(x_n, w) - t_n\right)^{2} + \frac{\lambda}{2} \left\Vert w\right\Vert_{}^{2}
\end{align}</span></p>
<p>Maximizing the likelihood of the posterior distribution is equivalent to the minimization problem associated with regularized least squares with regularization parameter <span class="math notranslate nohighlight">\(\lambda=\alpha/\beta\)</span>.</p>
<p><strong>Fully Bayesian treatment</strong></p>
<p>Although in the previous section we introduced a prior on <span class="math notranslate nohighlight">\(w\)</span>, we are still making a point estimate of <span class="math notranslate nohighlight">\(w\)</span>. For a fully Bayesian treatment, we should integrate over all values of <span class="math notranslate nohighlight">\(w\)</span>. We will cover this in much more detail later; for now, we provide only a sketch of the idea and the final result.</p>
<p>Our inputs are the training data set <span class="math notranslate nohighlight">\((x,t)_{n=1}^N\)</span> and given a new point <span class="math notranslate nohighlight">\(x^*\)</span>, we would like to find a predictive distribution for <span class="math notranslate nohighlight">\(t^*\)</span>, <span class="math notranslate nohighlight">\(P(t^*|x^*,x,t)\)</span>. Applying the factorization rules for <span class="math notranslate nohighlight">\(w\)</span></p>
<p><span class="math">\begin{equation*}
P(t^*|x^*,x,t) = \int P(t^*|x^*, w) P(w|x,t)\, dw
\end{equation*}</span></p>
</div>
</div>
<div class="section" id="Model-selection">
<h2>1.5. Model selection<a class="headerlink" href="#Model-selection" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="Appendix">
<h2>1.6. Appendix<a class="headerlink" href="#Appendix" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="References">
<h2>1.7. References<a class="headerlink" href="#References" title="Permalink to this headline">¶</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="PRML_ch3_lin_reg.html" class="btn btn-neutral float-right" title="2. Linear models for regression" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="bayesian_inference_ch1_single_parameter_models.html" class="btn btn-neutral float-left" title="3. Introduction to single parameter models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Corbin Foucart

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>