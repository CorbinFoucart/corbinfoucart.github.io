

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>1. Introduction &mdash; NMSMExp 0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"equationNumbers": {"autoNumber": "AMS", "useLabelIds": true}}, "tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Multiparameter models" href="bayesian_inference_ch1_multiparameter_models.html" />
    <link rel="prev" title="7. Statistical estimation" href="convex_ch7_statistical_estimation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> NMSMExp
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Linear Algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#probability-and-statistics">Probability and Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#convex-optimization">Convex Optimization</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html#bayesian-inference">Bayesian Inference</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">1. Introduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#The-Bayesian-and-frequentist-viewpoints">1.1. The Bayesian and frequentist viewpoints</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Analogy-to-computer-functions">1.1.1. Analogy to computer functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Practical-considerations">1.1.2. Practical considerations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Bayesian-framework">1.2. Bayesian framework</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Bayes-Theorem-for-an-event">1.2.1. Bayes Theorem for an event</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Bayes’-theorem-for-random-variables">1.2.2. Bayes’ theorem for random variables</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Example:-Finding-the-bias-of-an-unfair-coin">1.3. Example: Finding the bias of an unfair coin</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Analytical-solution-via-conjugate-priors">1.3.1. Analytical solution via conjugate priors</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Example:-Inference-of-a-distribution-parameter">1.4. Example: Inference of a distribution parameter</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Quick-review-of-the-Poisson-distribution">1.4.1. Quick review of the Poisson distribution</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="bayesian_inference_ch1_multiparameter_models.html">2. Multiparameter models</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesian_inference_ch1_single_parameter_models.html">3. Introduction to single parameter models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#machine-learning">Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#gaussian-processes">Gaussian Processes</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NMSMExp</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Linear Algebra</a> &raquo;</li>
        
      <li>1. Introduction</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorial/bayesian_inference_ch1_intro.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;bmh&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="Introduction">
<h1>1. Introduction<a class="headerlink" href="#Introduction" title="Permalink to this headline">¶</a></h1>
<div class="section" id="The-Bayesian-and-frequentist-viewpoints">
<h2>1.1. The Bayesian and frequentist viewpoints<a class="headerlink" href="#The-Bayesian-and-frequentist-viewpoints" title="Permalink to this headline">¶</a></h2>
<p>The Bayesian approach interprets probabilities as the measure of the <em>believability of an event</em>, that is, how confident we are in an event occurring.</p>
<p>A <strong>frequentist</strong> approach interprets probabilities as the long-run frequency of the event occurring. That is, an event’s probability is the limit of its relative frequency after many trials. This sometimes makes sense. The probability of flipping heads with a fair coin can be interpreted as the frequency of heads occurring if we did infinite flips. However, for something like an election, we would like to assign a probability of a candidate winning, even if we can do the election only once.
Frequentists get around this by saying if you <em>could</em> repeat the experiment, the frequency with the event occurred after infinite trials would represent the event probability. However for real world examples that occur once, this is a bit squirrely, since exactly repeating the experiment should reproduce the same result. This introduces all sorts of rabbit holes that we will ignore.</p>
<p>The <strong>Bayesian</strong> approach, by comparison, is a lot more intuitive. Bayesians interpret probabilities as the belief, or confidence, that an event will occur. This removes the need for many experiments and maps nicely to how the human brain interprets probabilities.</p>
<div class="section" id="Analogy-to-computer-functions">
<h3>1.1.1. Analogy to computer functions<a class="headerlink" href="#Analogy-to-computer-functions" title="Permalink to this headline">¶</a></h3>
<p>If you were to consider Bayesian and frequentist inference as programming functions, both would take a statistical problem as an input, but would return different outputs. The frequentist function would return <em>a number representing an estimate</em> (usually a summary statistic or expected value). The Bayesian function would return <em>probabilities</em>. The Bayesian function could additionally take as an optional argument a <em>prior belief</em> about the outcome.</p>
<p><strong>Example</strong> &gt; Suppose each program was to estimate whether a patient has a disease given <span class="math notranslate nohighlight">\(X\)</span> symptoms. The frequentist function would output an answer like <code class="docutils literal notranslate"><span class="pre">NO</span></code>. The Bayesian program would output probabilities over the outcomes, like <code class="docutils literal notranslate"><span class="pre">NO</span> <span class="pre">with</span> <span class="pre">probability</span> <span class="pre">0.8,</span> <span class="pre">YES</span> <span class="pre">with</span> <span class="pre">a</span> <span class="pre">probability</span> <span class="pre">0.2</span></code>.</p>
<p>Bayesian inference can be thought of as re-weighting prior beliefs according to new evidence.</p>
</div>
<div class="section" id="Practical-considerations">
<h3>1.1.2. Practical considerations<a class="headerlink" href="#Practical-considerations" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Frequentist approaches are still very useful, and tend to be computationally much cheaper than Bayesian approaches.</p></li>
<li><p>Bayesian approaches attempt to offer a more comprehensive view of beliefs, but can be much more expensive.</p></li>
<li><p>Which technique to use is largely problem dependent.</p></li>
</ul>
</div>
</div>
<div class="section" id="Bayesian-framework">
<h2>1.2. Bayesian framework<a class="headerlink" href="#Bayesian-framework" title="Permalink to this headline">¶</a></h2>
<div class="section" id="Bayes-Theorem-for-an-event">
<h3>1.2.1. Bayes Theorem for an event<a class="headerlink" href="#Bayes-Theorem-for-an-event" title="Permalink to this headline">¶</a></h3>
<p>Suppose we are interested in the probability of an event <span class="math notranslate nohighlight">\(E\)</span> occurring. We start with a prior belief <span class="math notranslate nohighlight">\(P(E)\)</span>. We then make an observation <span class="math notranslate nohighlight">\(Y\)</span>. We want to know how likely it was that <span class="math notranslate nohighlight">\(E\)</span> occurs, given that we observed <span class="math notranslate nohighlight">\(Y\)</span></p>
<p><span class="math">\begin{align}
P(E|Y) = \frac{P(Y|E)P(E)}{P(Y)}
\end{align}</span></p>
<p>where - <span class="math notranslate nohighlight">\(P(E)\)</span> is the prior: what we believe before making an observation. - <span class="math notranslate nohighlight">\(P(Y|E)\)</span> is the likelihood: the probability of observing <span class="math notranslate nohighlight">\(Y\)</span> if <span class="math notranslate nohighlight">\(E\)</span> occurs. - <span class="math notranslate nohighlight">\(P(Y)\)</span> is the normalizing factor: the probability that we observe <span class="math notranslate nohighlight">\(Y\)</span> to begin with. - <span class="math notranslate nohighlight">\(P(E|Y)\)</span> is the posterior probability (output): the probability that <span class="math notranslate nohighlight">\(E\)</span> occurs, given that we’ve seen <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<div class="section" id="Example:-drug-testing">
<h4>1.2.1.1. Example: drug testing<a class="headerlink" href="#Example:-drug-testing" title="Permalink to this headline">¶</a></h4>
<p>Suppose that 0.5% of a population uses a particular drug. The test for this particular drug is 99% accurate, which we take to mean that the test will produce a positive result for 99% of drug users, and a negative result for 99% of non-users.</p>
<p>A guy walks into your clinic and tests positive for the drug. What is the probability that he has actually used the drug? Let <span class="math notranslate nohighlight">\(+\)</span> and <span class="math notranslate nohighlight">\(-\)</span> denote the events of a positive and negative test, respectively.</p>
<p><span class="math">\begin{align}
P(\text{drug user} \mid +)
&= \frac{P(\text{drug user}) P(+\mid \text{drug user})}{P(+)} \\
&= \frac{P(\text{drug user}) P(+\mid \text{drug user})}{P(+\mid \text{drug user})P(\text{drug user}) + P(+\mid\text{not drug user})P(\text{not drug user}) } \\
&= \frac{(0.99)(0.005)}{(0.99)(0.005) + (0.01)(0.995)}  \\
&= 0.332
\end{align}</span></p>
<p>This is surprising to most people; that son of a bitch walked into your clinic and tested positive on your highly accurate test, yet it’s more likely that he <em>isn’t</em> a drug user. What’s going on?</p>
<p>The intuition gap here comes from the prior. It was way more likely that he didn’t user drugs to begin with, which skews the probability towards a false positive.</p>
<p>The explicit way to think about it is out of 1000 people, 5 will be drug users. Almost certainly, they’ll all test positive. Out of the other 995 that aren’t users, 9.95 <span class="math notranslate nohighlight">\(\sim 10\)</span> will test positive due to the <span class="math notranslate nohighlight">\(1%\)</span> error of the test. Only 5 users out of the 15 positives are actual drug users <span class="math notranslate nohighlight">\(\sim 0.33\)</span> probability of a positive indicating drug usage, as we calculated.</p>
</div>
</div>
<div class="section" id="Bayes’-theorem-for-random-variables">
<h3>1.2.2. Bayes’ theorem for random variables<a class="headerlink" href="#Bayes’-theorem-for-random-variables" title="Permalink to this headline">¶</a></h3>
<p>The statement of Bayes’ theorem in terms of events can quickly become unwieldy when there are lots of non-simple events. The nice thing is that the theorem holds for random variables and distributions, which will be much more powerful.</p>
<p>We use <span class="math notranslate nohighlight">\(\theta\)</span> to denote parameters of interest that we want to infer. We use <span class="math notranslate nohighlight">\(y\)</span> to denote observed data. Then the <em>joint distribution</em> of <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(y\)</span> is <span class="math notranslate nohighlight">\(p(\theta,y)\)</span> and can be factored into conditional distributions</p>
<p><span class="math">\begin{equation}
p(\theta,y) = p(\theta)p(y|\theta) = p(y)p(\theta|y)
\end{equation}</span></p>
<p>From where we can write Bayes’ rule for the <em>posterior</em> distribution</p>
<p><span class="math">\begin{equation}
p(\theta|y) = \frac{p(\theta)p(y|\theta)}{p(y)}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(p(\theta)\)</span> has the interior of the <em>prior</em> distribution, that is, what we believe about <span class="math notranslate nohighlight">\(\theta\)</span> without any obserations <span class="math notranslate nohighlight">\(y\)</span>. Sometimes it’s useful to write the denominator as a function of <span class="math notranslate nohighlight">\(\theta\)</span> as well by summing over all its possible values, loosely:</p>
<p><span class="math">\begin{equation}
p(\theta|y) = \frac{p(\theta)p(y|\theta)}{p(y)}
= \frac{p(\theta)p(y|\theta)}{\sum_{\theta}^{} p(\theta)p(y|\theta)}
\end{equation}</span></p>
<p>Usually we don’t care about the normalization factor though; it’s typically expensive or not possible to compute. What we generally care about is the shape of the posterior, so we can also ignore the normalization factor and write</p>
<p><span class="math">\begin{equation}
p(\theta|y) \propto p(\theta)p(y|\theta)
\end{equation}</span></p>
<p>That’s it. In principle this is all to the core of Bayesian inference. If we can form these objects well, we can do inference.</p>
<div class="section" id="Linking-example:-being-a-carrier-for-Hemophilia">
<h4>1.2.2.1. Linking example: being a carrier for Hemophilia<a class="headerlink" href="#Linking-example:-being-a-carrier-for-Hemophilia" title="Permalink to this headline">¶</a></h4>
<p>This example links the event discription and the distribution description.</p>
<p><strong>Biological aside:</strong></p>
<p>Males have XY chromosomes and females have XX chromosomes. Hemophilia is X-linked; if you’re male and your only X chromosome has the hemophilia gene, you’re affected. If you’re female, you generally can’t have hemophilia. You either have neither X chromosome affected (not a carrier) or have a single X chromosome with the hemophilia gene (carrier but not affected). If a female has both chromosomes with hemophilia, they are affected but usually die, so we exclude that segment of the population.</p>
<p><strong>Question of interest</strong></p>
<p>Suppose a woman has a brother affected by hemophilia. Then her mother certainly was a carrier, and she has a 50% probability of being a carrier. For each son she has, she gets more information on the probability that she is a carrier.</p>
<p>Let <span class="math notranslate nohighlight">\(\theta\)</span> be a random variable indicating whether she is a carrier; <span class="math notranslate nohighlight">\(\theta=0\)</span> is the probability that she isn’t a carrier and <span class="math notranslate nohighlight">\(\theta=1\)</span> is the probability that she is. Each son she has is either affected (<span class="math notranslate nohighlight">\(y_i=1\)</span>) or not <span class="math notranslate nohighlight">\((y_i=0)\)</span>.</p>
<p>Before the woman has any sons, clearly the probability of her being a carrier is 50/50, which we encapsulate with the prior <span class="math notranslate nohighlight">\(p(\theta=1)=0.5\)</span>.</p>
<p><strong>Case 1: first son affected</strong></p>
<p>If any son is affected, she is clearly a carrier, and the math works. Suppose the first son is affected.</p>
<p><span class="math">\begin{align}
p(\theta=1 | y_1=1)
&= \frac{p(y_1=1|\theta=1)p(\theta=1)}{p(y_1=1)} \\
&= \frac{p(y_1=1|\theta=1)p(\theta=1)}{p(y_1=1|\theta=1)p(\theta=1)+p(y_1=1|\theta=0)p(\theta=0)} \\
&= \frac{(0.5)(0.5)}{(0.5)(0.5) + (0)(0.5)} = 1
\end{align}</span></p>
<p><strong>Case 2: first son not affected</strong></p>
<p>However, if instead her first son is unaffected <span class="math notranslate nohighlight">\(y_1 = 0\)</span>:</p>
<p><span class="math">\begin{align}
p(\theta=1 | y_1=0)
&= \frac{p(y_1=0|\theta=1)p(\theta=1)}{p(y_1=0)} \\
&= \frac{p(y_1=0|\theta=1)p(\theta=1)}{p(y_1=0|\theta=1)p(\theta=1)+p(y_1=0|\theta=0)p(\theta=0)} \\
&= \frac{(0.5)(0.5)}{(0.5)(0.5) + (1)(0.5)} = 0.33
\end{align}</span></p>
<p><strong>Sequential data assimilation</strong></p>
<p>One of the nice things about the Bayesian approach is that as more data flows in, we don’t have to start from the beginning. Our posterior distribution before the new data just becomes the new prior!</p>
<p>Suppose she has a second unaffected son. How does her probability of being a carrier change? The math is the same, only the prior is different.</p>
<p><span class="math">\begin{align}
p(\theta=1 | y_1=0)
&= \frac{p(y_1=0|\theta=1)p(\theta=1)}{p(y_1=0)} \\
&= \frac{p(y_1=0|\theta=1)p(\theta=1)}{p(y_1=0|\theta=1)p(\theta=1)+p(y_1=0|\theta=0)p(\theta=0)} \\
&= \frac{(0.5)(1/3)}{(0.5)(1/3) + (1)(2/3)} = 0.2
\end{align}</span></p>
<p>So the likelihood of her being a carrier has further decreased.</p>
<p>In fact, we can even write code to sequentially compute her probability of being a carrier for every subsequent nonaffected son.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">p_yθ_</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># probabilities matrix; y is row, θ is col</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.5</span><span class="p">],</span>
                           <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">]])</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">θ</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="n">y</span><span class="p">,</span><span class="n">θ</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">carrier_likelihood</span><span class="p">,</span> <span class="n">trials</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="mi">12</span>
<span class="n">pθ</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">pyθ</span> <span class="o">=</span> <span class="n">p_yθ_</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">trials</span><span class="p">):</span>
    <span class="n">num</span> <span class="o">=</span> <span class="n">pyθ</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">θ</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">pθ</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="n">pyθ</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">θ</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">pθ</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">pyθ</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">θ</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">*</span><span class="n">pθ</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">posterior_probability</span> <span class="o">=</span> <span class="n">num</span><span class="o">/</span><span class="n">denom</span>
    <span class="n">carrier_likelihood</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">posterior_probability</span><span class="p">)</span>

    <span class="c1"># update our prior belief</span>
    <span class="n">pθ</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="o">-</span><span class="n">posterior_probability</span><span class="p">,</span> <span class="n">posterior_probability</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">trials</span><span class="p">),</span> <span class="n">carrier_likelihood</span><span class="p">,</span> <span class="s1">&#39;--o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;carrier prob`ability vs. n unaffected sons&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_bayesian_inference_ch1_intro_8_0.png" src="../_images/tutorial_bayesian_inference_ch1_intro_8_0.png" />
</div>
</div>
<p>With each subsequent unaffected son, the probability of the woman being a carrier drops. Unfortunately, she’d have to have around 7 sons to be 99% sure she’s not a carrier!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../data/gelman_football_data_cleaned.txt&#39;</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot;\s+&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">outcomes</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;favorite&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;underdog&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">point_spread</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;spread&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">point_spread</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">outcomes</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;point spread&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;outcome&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_bayesian_inference_ch1_intro_11_0.png" src="../_images/tutorial_bayesian_inference_ch1_intro_11_0.png" />
</div>
</div>
<p>What is the empirically the probability that a favorite will win the game?</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">favorite_won</span> <span class="o">=</span> <span class="n">outcomes</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">favorite_won</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">favorite_won</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>0.6584821428571429
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="Example:-Finding-the-bias-of-an-unfair-coin">
<h2>1.3. Example: Finding the bias of an unfair coin<a class="headerlink" href="#Example:-Finding-the-bias-of-an-unfair-coin" title="Permalink to this headline">¶</a></h2>
<p>Suppose, naively, that you are unsure about the probability of heads in a coin which you supect could be biased (here it’s 65%). You believe there is some true underlying ratio, call it <span class="math notranslate nohighlight">\(p\)</span>, but have no prior opinion on what <span class="math notranslate nohighlight">\(p\)</span> might be.</p>
<p>We begin to flip a coin, and record the observations: either <span class="math notranslate nohighlight">\(H\)</span> or <span class="math notranslate nohighlight">\(T\)</span>. This is our observed data. An interesting question to ask is how our inference changes as we observe more and more data–more specifically, what do our posterior probabilities look like when we have little data, versus when we have lots of data.</p>
<div class="section" id="Analytical-solution-via-conjugate-priors">
<h3>1.3.1. Analytical solution via conjugate priors<a class="headerlink" href="#Analytical-solution-via-conjugate-priors" title="Permalink to this headline">¶</a></h3>
<p>The thing we are trying to estimate is the parameter <span class="math notranslate nohighlight">\(p\)</span>. Out of <span class="math notranslate nohighlight">\(n=s + f\)</span> trials, we have <span class="math notranslate nohighlight">\(s\)</span> successes (H) and <span class="math notranslate nohighlight">\(f\)</span> failures (T). We know the probability of observing <span class="math notranslate nohighlight">\(s\)</span> heads given a choice of <span class="math notranslate nohighlight">\(p\)</span>; it’s the Binomial distribution. This is the likelihood function</p>
<p><span class="math">\begin{equation}
P(s|p) = { s+f \choose s} p^s (1-p)^f.
\end{equation}</span></p>
<p>We have a choice as to what our prior belief should be. A reasonable choice is a uniform distribution over all possible values of <span class="math notranslate nohighlight">\(p\)</span> on <span class="math notranslate nohighlight">\([0,1]\)</span>. But we can do even better. We can choose a well known <strong>conjugate prior</strong> of the likelihood, which means that we choose a prior such that when we compute the posterior</p>
<p><span class="math">\begin{equation}
P(p|s) \propto P(s|p)P(p),
\end{equation}</span></p>
<p>it has the same form as the prior we started with. This is useful for continuous updates, because we can then take the posterior as our new prior and repeat the same procedure, confident that the algebra will be the same.</p>
<p>Let’s make this concrete. We drop all normalizing factors since we’ll just be interested in plotting the distribution shape. Suppose we choose a prior according to a Beta distribution</p>
<p><span class="math">\begin{equation}
\text{Beta}(\alpha,\beta) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}
\propto  x^{\alpha-1}(1-x)^{\beta-1},
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(B(\alpha,\beta)\)</span> is a constant that comes from evaluating the Gamma function. We can compute the unnormalized posterior directly as</p>
<p><span class="math">\begin{align}
P(p|s) &\propto
{ s+f \choose s} p^s (1-p)^f
p^{\alpha-1}(1-p)^{\beta-1}, \\
&\propto p^{(s+\alpha)-1}(1-p)^{(f+\beta)-1},
\end{align}</span></p>
<p>which is a new Beta distribution with parameters <span class="math notranslate nohighlight">\((s+\alpha,\, f+\beta)\)</span>. The posterior distribution can be used as a prior for more samples, with <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> adding additional information as it comes in.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">Beta</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span>
<span class="n">p_true</span> <span class="o">=</span> <span class="mf">0.65</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">300</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">trials</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">3000</span><span class="p">]</span> <span class="c1"># even for figure</span>
<span class="n">simulation</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">bernoulli</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">p_true</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">trials</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>A frequentist estimate of the bias of the coin could be found by simply taking the average of the array, which contains the zeroes and ones of the simulation. Extremely cheap, but doesn’t give us the uncertainty.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">simulation</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">trials</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trials</span><span class="p">):</span>

    <span class="n">tosses</span> <span class="o">=</span> <span class="n">simulation</span><span class="p">[:</span><span class="n">N</span><span class="p">]</span>
    <span class="n">heads_so_far</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">tosses</span><span class="p">)</span>
    <span class="n">tails_so_far</span> <span class="o">=</span> <span class="n">N</span> <span class="o">-</span> <span class="n">heads_so_far</span>
    <span class="n">α</span><span class="p">,</span> <span class="n">β</span> <span class="o">=</span> <span class="n">heads_so_far</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tails_so_far</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">Beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">α</span><span class="p">,</span> <span class="n">β</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> tosses, </span><span class="si">{}</span><span class="s1"> heads&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">heads_so_far</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">p_true</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">posterior</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>


<span class="c1"># plot aesthetics</span>
<span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="n">ax</span><span class="p">:</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([]);</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">autoscale</span><span class="p">(</span><span class="n">tight</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;posterior distributions for biased coin&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.45</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p><strong>remarks</strong> - before we’ve seen any information, if we initialize with <span class="math notranslate nohighlight">\(\alpha=\beta=1\)</span> that the prior distribution is uniform! - the Bayesian description of the coin does better than a frequentist estimation in the sense that it also shows how confident we are in our estimate. - Initially, with ~10 tosses, our posterior is still pretty uncertain, we can’t really say much of anything without more data. - After only 256 tosses, it’s highly likely that the coin is biased. After 3000 tosses
we’re basically certain that the coin is biased. - coming up with an analyitcal solution to an inference problem only works in very special (toy problem) cases; we need other approaches for real problems.</p>
</div>
</div>
<div class="section" id="Example:-Inference-of-a-distribution-parameter">
<h2>1.4. Example: Inference of a distribution parameter<a class="headerlink" href="#Example:-Inference-of-a-distribution-parameter" title="Permalink to this headline">¶</a></h2>
<div class="section" id="Quick-review-of-the-Poisson-distribution">
<h3>1.4.1. Quick review of the Poisson distribution<a class="headerlink" href="#Quick-review-of-the-Poisson-distribution" title="Permalink to this headline">¶</a></h3>
<p>The Poisson distribution is a discrete probability distribution which is used to model the probability of a givne number of events occurring in a fixed interval of space or time.</p>
<p><strong>Assumptions</strong> &gt; The Poisson distribution is a good model if - The occurrence of one event does not change the probability that another event will occur (events are indpendent). - The average rate of event occurrence does not change.</p>
<p>The distribution is defined over <span class="math notranslate nohighlight">\(\mathbb{N}_0\)</span> and has PMF</p>
<p><span class="math">\begin{equation}
\text{Pois}(\lambda) \sim \frac{\lambda^k}{k!} e^{-\lambda} \qquad k=0,1,2,\ldots
\end{equation}</span></p>
<p>if <span class="math notranslate nohighlight">\(Z\sim \text{Pois}(\lambda)\)</span>, then <span class="math notranslate nohighlight">\(E[Z] = \lambda\)</span> and <span class="math notranslate nohighlight">\(Var(Z) = \lambda\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">Pois</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span>
<span class="n">ks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
<span class="n">λs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">4.25</span><span class="p">]</span>

<span class="k">for</span> <span class="n">λ</span> <span class="ow">in</span> <span class="n">λs</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">Pois</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">λ</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\lambda=</span><span class="si">{}</span><span class="s1">$&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">λ</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;PMF of Pois($\lambda$), for different $\lambda$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="bayesian_inference_ch1_multiparameter_models.html" class="btn btn-neutral float-right" title="2. Multiparameter models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="convex_ch7_statistical_estimation.html" class="btn btn-neutral float-left" title="7. Statistical estimation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Corbin Foucart

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>